\documentclass[11pt]{article}
\usepackage{times}

\title{CMSC621 Project 1 Report}
\author{Patrick Trinkle \& Mike Corbin\\
Dept. of Computer Science and Electrical Engineering,\\
University of Maryland Baltimore County,\\
Baltimore, MD, 21250\\
\texttt{(tri1|corbin2)@umbc.edu}}
\date{October 2009}

\begin{document}
\maketitle

\section{Introduction}
The assignment was to implement a distributed solution to the problem of calculating information about a large file F, comprised of double precision floating point values.  ProjectOne will quickly determine the maximum, minimum, average and median values as well as the count.

\section{Background}
A distributed system is a processing system where the computation is spread out over a series of systems.  These systems can be controlled in many ways, possibly via a primary tasking node or alternatively in a peer-to-peer tasking scenario.  With the central tasking model you can easily imagine a large task being broken into independent pieces and these tasks being assigned to a plethora of machines all over the network.  Processing redundancy as well as good bookkeeping can allow this type of processing to easily restart on a node if it goes down.  If a node is no longer communicating properly (possibly because it is dead) the tasking assigned to it can be shifted to another node.  Therefore the processing is still being handled in a distributed way that is semi-fault tolerant.  As long as the cost of sending the data to be processed is less then the time to process it, this is a good solution to a large problem.  Because of this cost in data passing, not all problems that can be parallelized truly benefit from such systems.  However, for the most part--processing that can be parallelized should be.  To aid in taking a problem and solving it in a distributed way there are infrastructures in place that a developer can utilize.  Hadoop is an implementation of a distributed platform, for which a developer can write code that will work in a distributed way with most of the details hidden.  Using a pre-existing platform can save a lot of time and money in developing distributed applications.

\subsection*{Hadoop}
Our system is implemented using Hadoop.  To manage the input and output files our system relies on the Hadoop Distributed File System (HDFS).  The HDFS will automatically replicate files across nodes and manage file coherency.  The program interfaces with Hadoop's implementation of MapReduce.

\subsection*{MapReduce}
MapReduce is an algorithm by which a problem is broken up into two primary steps: the map step; and the reduce step.  During the map step there are some actions on the input that produce key value pairs.  During the reduce step each reducer will handle strictly one key and the list of data associated with that key.  The reduce step may very well produce input for another iteration of the algorithm.  The reduce step also outputs one file per key in the system.  If the input was all instances of a word then the reduce step could output the number of instances of each of the words (keys).  Hadoop provides an abstraction/interface for building programs that utilize MapReduce.  Part of what is provided is file handling.  Hadoop will split a file and send it out across the available nodes during the mapping step automatically.  A developer can for the most part modify an interface to this file splitting to leverage how much is sent at a time.  The keys value pairs from the mapping step are automatically captured by the system and stored in an intermediate storage location; and sorted by key.  Each key is mapped to a reducer process that can be on any of the nodes.  If the map step returns many different keys the processing is spread out over equally many reducers.

\section{Implementation}
We first implemented our own import format class that can read in multiple lines at a time.  We did this because are input file contains one number on each line, and if we only read one line at a time the first mapping phase could do very little work.  By reading in multiple lines can do more work in the initial mapping phase which lets us find each answer in one map reduce cycle.

We implemented three sets of mappers/reducers to solve maxmin, average, and median independently.  This allowed for less frequent type conversions; because the average mapper needed to output Text--whereas the maximin mapper output DoubleWritables.  Because we get the maximum and the minimum values for free when calculating the average it was decided that this difference was significant enough to combine the mapper.

We implemented two sets of mapppers/reducers for this project.  The first set determines the maximum, minimum,  and average value of the file (as well as the count).  The mapper finds the max/min/avg of the set of numbers it is give and returns each as a key/value pair.  The reduce then finds the max/min/avg out of all these key value pairs.  Since we are able to send multiple rows to the mapper we can find the max/min/avg in one map reduce step.  The average is special in that the output is actually a sum and a count in a specified format for processing by the reducer.

Out final set of mappers/reducers estimates the median by calculating the median of the medians.  Both our map and reduce functions do this continually reducing the set by finding the median of each 5 elements until there are 5 or less elements left.  It then returns the median of the remaining 5 elements.  This process can be done very fast using map reduce because the input can be broken into as small as 5 element pieces before being processed.  In the worst case this process splits the data by 30/70 percent.  In general it does much better than this because each estimate is less than 2 elements and greater than 2 others.  After many tests on large randomized data sets the estimated median was a good representation of the split between the higher and lower values in the file.

\section{Running}

Our jar file is named projects.jar and it contains a proj package with class ProjectOne.  The main class takes in 4 paramaters which are listed below.\\\\
Hadoop jar projects.jar proj.ProjectOne [type] [file location] [input file] [output folder]\\
newline type: maxminavg, med\\
file location: local, hdfs\\
input file: path to input file\\
output folder: output folder path (on hadoop, output will be copied to home directory on local node)\\

On the first run of the process the input file is copied into the HDFS (if necessary).  Because the HDFS allows access to the results data from every node with abstracted costs, it is sufficient to place the results in the HDFS.  One of the primary features of using Hadoop as a distributed processing system is that it abstracts away most of the core functionality so that a programmer/user can build a simpler project interfacing with the Hadoop API.

\subsection*{Examples}
Hadoop jar projects.jar proj.ProjectOne maxminavg local numbers results\\
Hadoop jar projects.jar proj.ProjectOne med hdfs numbers results

\end{document}
